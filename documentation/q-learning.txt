Mock code for q-learning

# 0. Initialize

##train.py

state_count  = 81   # number of possible feature states, currently 81
action_count = len(ACTIONS)   # 6

setup_training()
    # Initialize Q
    self.Q = np.zeros((state_count, action_count))   # initial guess for Q, for now just zeros
    
    self.training_data = []   # [[features, action_index, reward], ...]
    self.state_occurances = np.zeros_like(self.Q)   # a counter for how often the individual game_states happened during training



# 1. Train

# loop over games handled in train mode
game_count = ...  # number of training games


# 1.a Play one round
##callbacks.py

def epsilon_greedy (recommended_action, epsilon):
    random_action = np.random.choice(ACTIONS)

    return np.random.choice([recommended_action, random_action], p = [1 - epsilon, epsilon])

def find_state (features):
    # currently just assigns one state to each unique features
    # currently, there are 3^4 = 81 different states
    # every point in the feature space gets assigned one number

    return features[0]*3**3 + features[1]*3**2 + features[2]*3 + features[3]   

act(game_state)
    features    = state_to_features(game_state)
    state_index = find_state(features)
    
    policy        = np.argmax(self.Q[state_index])
    policy_action = ACTIONS[policy]
    
    return epsilon_greedy(policy_action)


# 1.a Collect training data
##train.py

game_events_occured():
    features = state_to_features(game_state)
    action   = np.find(ACTIONS, self_action) # find index of self_action
    reward   = new_game_state['self'][1] - old_game_state['self'][1]   # just game reward for now, reward_from_events() better place for training reward calculations

    self.training_data.append([features, action, reward])


# 1.b Update Q-function
##train.py

from .callbacks import find_state

# hyperparameters for Q-update
alpha = 1
gamma = 1

end_of_round()
        # no update to self.training_data here (game_events_occured() also gets called after the last action, doesn't it?)
    
    game_length = len(self.training_data)

    features_old, action_old, reward_new    = self.training_data[0]
    state_index_old                         = find_state(features_old)
    self.state_occurances[state_index_new] += 1
    
    for step in range(1, game_length):
        # Preparation
        features_new, action_new, reward_next   = self.training_data[step]
        state_index_new                         = find_state(features_new)
        self.state_occurances[state_index_new] += 1

        Q_state_old  = self.Q[state_index_old][action_old]
        Q_state_new  = self.Q[state_index_new][action_new]
        V_state_new  = np.max(Q_state_new)   # implemented Q-learning instead of SARSA
        N_Sa         = self.state_occurances[state_index_new]
        
        # Q-Update
        self.Q[state_index_old][action_old] = Q_state_old + alpha / N_Sa  * (reward_new + gamma* V_state_new - Q_state_old)

        # new state becomes old state
        state_index_old = state_index_new
        action_old      = action_new
        reward_new      = reward_next
        

    # Save updated Q-function as new model
    self.model = self.Q
    with open(model_file, "wb") as file:
        pickle.dump(self.model, file)


# 2. Test
##callbacks.py

setup()
    with open("my-saved-model.pt", "rb") as file:
        self.model = pickle.load(file)

act()
    features    = state_to_features(game_state)
    state_index = find_state(features)
    
    policy        = np.argmax(self.model[state_index])
    policy_action = ACTIONS[policy]
    
    return policy_action







a propos find_state():

    '''
    (x, y)  3x3 Bild
    Q.shape = (9, 6)
    Q[x*3 + y]

    feature -> Q_index
    0 3 6
    1 4 7
    2 5 8

    Q = 
    [[state 0 actions],
     [state 1 actions],
     ...
    ]
    '''