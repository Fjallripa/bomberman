{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d57c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load callbacks.py\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "ACTIONS = ['UP', 'RIGHT', 'DOWN', 'LEFT', 'WAIT', 'BOMB']\n",
    "\n",
    "\n",
    "def setup(self):\n",
    "    \"\"\"\n",
    "    Setup your code. This is called once when loading each agent.\n",
    "    Make sure that you prepare everything such that act(...) can be called.\n",
    "\n",
    "    When in training mode, the separate `setup_training` in train.py is called\n",
    "    after this method. This separation allows you to share your trained agent\n",
    "    with other students, without revealing your training code.\n",
    "\n",
    "    In this example, our model is a set of probabilities over actions\n",
    "    that are independent of the game state.\n",
    "\n",
    "    :param self: This object is passed to all callbacks and you can set arbitrary values.\n",
    "    \"\"\"\n",
    "    if self.train or not os.path.isfile(\"my-saved-model.pt\"):\n",
    "        self.logger.info(\"Setting up model from scratch.\")\n",
    "        weights = np.random.rand(len(ACTIONS))\n",
    "        self.model = weights / weights.sum()\n",
    "    else:\n",
    "        self.logger.info(\"Loading model from saved state.\")\n",
    "        with open(\"my-saved-model.pt\", \"rb\") as file:\n",
    "            self.model = pickle.load(file)\n",
    "\n",
    "\n",
    "def look_for_targets(free_space, start, targets, logger=None):\n",
    "    \"\"\"Find direction of the closest target that can be reached via free tiles.\n",
    "\n",
    "    Performs a breadth-first search of the reachable free tiles until a target is encountered.\n",
    "    If no target can be reached, the path that takes the agent closest to any target is chosen.\n",
    "\n",
    "    Args:\n",
    "        free_space: Boolean numpy array. True for free tiles and False for obstacles.\n",
    "        start: the coordinate from which to begin the search.\n",
    "        targets: list or array holding the coordinates of all target tiles.\n",
    "        logger: optional logger object for debugging.\n",
    "    Returns:\n",
    "        coordinate of first step towards the closest target or towards tile closest to any target.\n",
    "    \"\"\"\n",
    "    if len(targets) == 0:\n",
    "        return None\n",
    "    \n",
    "    frontier = [start]\n",
    "    parent_dict = {start: start}\n",
    "    dist_so_far = {start: 0}\n",
    "    best = start\n",
    "    best_dist = np.sum(np.abs(np.subtract(targets, start)), axis=1).min()\n",
    "\n",
    "    while len(frontier) > 0:\n",
    "        current = frontier.pop(0)\n",
    "        # Find distance from current position to all targets, track closest\n",
    "        d = np.sum(np.abs(np.subtract(targets, current)), axis=1).min()\n",
    "        if d + dist_so_far[current] <= best_dist:\n",
    "            best = current\n",
    "            best_dist = d + dist_so_far[current]\n",
    "        if d == 0:\n",
    "            # Found path to a target's exact position, mission accomplished!\n",
    "            best = current\n",
    "            break\n",
    "        # Add unexplored free neighboring tiles to the queue in a random order\n",
    "        x, y = current\n",
    "        neighbors = [(x_n, y_n) for (x_n, y_n) in [(x + 1, y), (x - 1, y), (x, y + 1), (x, y - 1)] if free_space[x_n, y_n]]\n",
    "        random.shuffle(neighbors)\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor not in parent_dict:\n",
    "                frontier.append(neighbor)\n",
    "                parent_dict[neighbor] = current\n",
    "                dist_so_far[neighbor] = dist_so_far[current] + 1\n",
    "    if logger: logger.debug(f'Suitable target found at {best}')\n",
    "    # Determine the first step towards the best found target tile\n",
    "    current = best\n",
    "    while True:\n",
    "        if parent_dict[current] == start: return current\n",
    "        current = parent_dict[current]\n",
    "\n",
    "\n",
    "def state_to_features(game_state: dict) -> np.array:\n",
    "    \"\"\"\n",
    "    *This is not a required function, but an idea to structure your code.*\n",
    "\n",
    "    Converts the game state to the input of your model, i.e.\n",
    "    a feature vector.\n",
    "\n",
    "    You can find out about the state of the game environment via game_state,\n",
    "    which is a dictionary. Consult 'get_state_for_agent' in environment.py to see\n",
    "    what it contains.\n",
    "\n",
    "    :param game_state:  A dictionary describing the current game board.\n",
    "    :return: np.array\n",
    "    \"\"\"\n",
    "    # This is the dict before the game begins and after it ends\n",
    "    if game_state is None:\n",
    "        return None\n",
    "\n",
    "    ### design features for Task 1 ###\n",
    "    \"\"\"\n",
    "    np.array where each component corresponds to on neighbour field\n",
    "    = 0 if wall or crate\n",
    "    = 1 if free\n",
    "    = 2 if free and (one) nearest field to nearest coin\n",
    "    \"\"\"\n",
    "    X = np.zeros(4) # hand-crafted feature vector\n",
    "    \n",
    "    free_space = game_state['field'] == 0 # Boolean numpy array. True for free tiles and False for Crates & Walls\n",
    "    agent_x, agent_y = game_state['self'][3] # Agent position as coordinates \n",
    "    coin_direction = look_for_targets(free_space, (agent_x, agent_y), game_state['coins']) # neighbouring field closest to closest coin\n",
    "\n",
    "    neighbours = [(agent_x + 1, agent_y), (agent_x - 1, agent_y), (agent_x, agent_y + 1), (agent_x, agent_y - 1)]\n",
    "    \n",
    "    for j, neighbour in enumerate(neighbours):\n",
    "        if neighbour == coin_direction: \n",
    "            X[j] = 2\n",
    "        elif free_space[neighbour[0], neighbour[1]]:\n",
    "            X[j] = 1\n",
    "    \n",
    "    return(X)    \n",
    "            \n",
    "            \n",
    "def act(self, game_state: dict) -> str:\n",
    "    \"\"\"\n",
    "    Your agent should parse the input, think, and take a decision.\n",
    "    When not in training mode, the maximum execution time for this method is 0.5s.\n",
    "\n",
    "    :param self: The same object that is passed to all of your callbacks.\n",
    "    :param game_state: The dictionary that describes everything on the board.\n",
    "    :return: The action to take as a string.\n",
    "    \"\"\"\n",
    "    # todo Exploration vs exploitation\n",
    "    random_prob = .1\n",
    "    if self.train and random.random() < random_prob:\n",
    "        self.logger.debug(\"Choosing action purely at random.\")\n",
    "        # 80%: walk in any direction. 10% wait. 10% bomb.\n",
    "        return np.random.choice(ACTIONS, p=[.2, .2, .2, .2, .1, .1])\n",
    "\n",
    "    self.logger.debug(\"Querying model for action.\")\n",
    "    return np.random.choice(ACTIONS, p=self.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc135a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where to go: [1. 0. 2. 0.]\n",
      "\n",
      "[[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [-1  8  0  3  0  0  0  0  0  0  0  0  0  0  0  0 -1]\n",
      " [-1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1]\n",
      " [-1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1]\n",
      " [-1  3 -1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1]\n",
      " [-1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1]\n",
      " [-1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1]\n",
      " [-1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1]\n",
      " [-1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1]\n",
      " [-1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1]\n",
      " [-1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1]\n",
      " [-1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1]\n",
      " [-1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1]\n",
      " [-1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1]\n",
      " [-1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1  0 -1]\n",
      " [-1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "# test -> successfull! :))\n",
    "\n",
    "def build_arena(COLS = 17, ROWS = 17):\n",
    "    WALL = -1\n",
    "    FREE = 0\n",
    "    arena = np.zeros((COLS, ROWS), int)\n",
    "\n",
    "    # Walls\n",
    "    arena[:1, :] = WALL\n",
    "    arena[-1:, :] = WALL\n",
    "    arena[:, :1] = WALL\n",
    "    arena[:, -1:] = WALL\n",
    "    for x in range(COLS):\n",
    "        for y in range(ROWS):\n",
    "            if (x + 1) * (y + 1) % 2 == 1:\n",
    "                arena[x, y] = WALL\n",
    "    \n",
    "    return(arena)\n",
    "\n",
    "field_test = build_arena()\n",
    "coins_test = [(4,1), (1,3)]\n",
    "start_test = (1,1)\n",
    "self_test = ('my_agent_1', 0, 0, start_test)\n",
    "game_state_test = {'field' : build_arena(), 'coins': coins_test, 'self': self_test}\n",
    "\n",
    "arena_test = build_arena() \n",
    "arena_test[start_test[0], start_test[1]] = 8\n",
    "for coin in coins_test:\n",
    "    arena_test[coin[0],coin[1]] = 3\n",
    "\n",
    "\n",
    "print(f\"Where to go: {state_to_features(game_state_test)}\" + \"\\n\")\n",
    "print(arena_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d362279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
